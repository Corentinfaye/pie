{
  // * General
  "verbose": true,
  "device": "cpu",
  // model name to be used for saving
  "modelname": "model",
  // model path to be used for saving
  "modelpath": "./",

  // * Data
  // path or unix-like expression to file(s)/dir with training data:
  // e.g. "datasets/capitula_classic/train0.tsv""
  "input_path": "",
  // path to test set (same format as input_path)
  "test_path": "",
  // path to dev set (same format as input_path)
  "dev_path": "",
  // proportion of instances in the dev set (ignored if dev_path is passed)
  "dev_split": 0.05,
  // breakline logic to use to detect sentence boundaries in tsv files
  // either "LENGTH" (break after n tokens) or "FULLSTOP" (break after a fullstop symbol)
  "breakline_type": "LENGTH",
  // data to use as reference for breaking lines (either "input" or a task name)
  "breakline_ref": "input",
  // needed to decide for a sentence boundary (e.g. an integer for LENGTH)
  "breakline_data": "15",
  // safety max length to avoid too long sentences blowing up memory
  "max_sent_len": 35,
  // maximum number of sentences to process
  "max_sents": 1000000,
  // maximum vocabulary size
  "char_max_size": 100,
  "word_max_size": 20000,
  // min freq per item to be incorporated in the vocabulary (only used if *_max_size is 0)
  "char_min_freq": 1,
  "word_min_freq": 1,
  // header
  "header": true,
  // expected order of tasks for tabreader
  "tasks_order": ["lemma", "pos"], // only for TabReader
  // task-related config
  "tasks": [
    // each task's name refers to the corresponding data field
    // this behaviour can be changed in case the name differs from the data field
    // by using a "target" key that refers to the target data field
    // e.g. {"name": "lemma-char", "settings": {"target": "lemma"}}
    // e.g. {"name": "lemma-word", "settings": {"target": "lemma"}}
    {
      // name (by default should match the target task)
      "name": "lemma",
      // whether the target is the target task (there can only be one)
      "target": true,
      // is this task token-level or char level? (token, char)
      "level": "char",
      // type of the decoder (linear, attentional, crf)
      "decoder": "attentional",
      // add sentential context (none, word, sentence, both). Only for char-level!
      "context": "none",
      // at what sentence encoder layer do we do this task?
      "layer": -1,
      // encoder settings ("max_size", "min_freq", "preprocessor")
      "settings": {
	"bos": true, "eos": true
      },
      // schedule: additional schedule params can be passed to control the extent
      // of learning of each task. There are 2 types of tasks: targets and slaves
      // depending on the value of "target" (if true then task is target else slave)
      // For target, "patience", "threshold" control early stopping (how many steps
      // without improvement over the threshold we need before stopping learning).
      // For auxiliary tasks, they control how we decay the loss weight of that task
      // and the extra parameters are: "factor" (by how much), "min_weight"
      // (minimum weight) given to the loss, "weight" (initial weight) and "mode"
      // (whether the task dev score is being minimized "min" or maximized "max")
      "schedule": {
	"patience": 2, "threshold": 0.001
      },
      // while processing the files if the field is missing use this instead
      // e.g. "copy": copy over the token form; "UNK": use "UNK"
      "default": "copy"
    },
    {
      "name": "pos", "default": "UNK"
    }
  ],

  // general task schedule params (can be overwritten in the "settings" entry of each)
  "patience": 1000000, // default to very large value
  "factor": 1, // decrease the loss weight by this amount (0, 1)
  "threshold": 0, // minimum decrease in loss to be considered an improvement
  "min_weight": 0, // minimum value a task weight can be decreased to

  // whether to include autoregressive loss
  "include_lm": false,
  // lm settings in case it's included as an extra task
  "lm_schedule": {
    "patience": 100, "factor": 0.5, "weight": 0.2, "mode": "min"
  },

  // * Training
  "buffer_size": 10000,
  // preprocess data to have similar sentence lengths inside batch
  "minimize_pad": false,
  "dropout": 0,
  "word_dropout": 0,
  "epochs": 5,
  "batch_size": 50,
  "shuffle": true,
  "optimizer": "Adam",
  "lr": 0.001,
  "lr_factor": 0.75,
  "lr_patience": 2,
  "clip_norm": 5.0,
  // print training loss every so many batches
  "report_freq": 10,
  // check model on dev-set so many times during epoch
  "checks_per_epoch": 1,
  // whether to use word2vec to initialize the embeddings
  "pretrain_embeddings": false,
  // path to file with word2vec pretrained embeddings
  "load_pretrained_embeddings": "",

  // * Model
  "wemb_dim": 150,
  // whether to freeze the word embeddings
  "freeze_embeddings": false,
  "cemb_dim": 150,
  // character embedding type (rnn or cnn)
  "cemb_type": "rnn",
  // whether to use the custom lstm cell for word embeddings
  "custom_cemb_cell": false,
  // how to merge word-level and char-level embeddings (mixer or concat)
  "merge_type": "concat",
  "hidden_size": 300,
  "num_layers": 1,
  "cell": "LSTM"
}
